{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"text-align:center\">Garrett Zimmerman</h3>\n",
    "<h3 style=\"text-align:center\">January 6, 2024</h3>\n",
    "<h1 style=\"font-weight:bold;text-align:center\">Concepts of Rag  Retrieval</h1>\n",
    "<h2 style=\"font-weight:bold\">Goal:</h2>\n",
    "<p>\n",
    "<dl>\n",
    "    <dt style=\"font-weight:bold\">RAG Retrieval:</dt> \n",
    "    <dd>Review the concept of RAG Retrieval<dd> \n",
    "    <dd>Question and Answering vs. Fine Tuning a Model<dd>\n",
    "    <dd>Advanced  Retrieval vs. Basic Retrieval</dd>\n",
    "    <dt style=\"font-weight:bold\">Build Several Examples:</dt> \n",
    "    <dd>Explain different advance retrieval methods\n",
    "        <ul>\n",
    "            <li>LlamaIndex</li>\n",
    "            <li>LangChain</li>\n",
    "            <li>Pinecone</li>\n",
    "            <li>ChromaDB</li>\n",
    "        </ul>\n",
    "    </dd> \n",
    "</dl>\n",
    "</p> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"font-weight:bold\">RAG (Retrieval-Augmented Generation) Retrieval:</h2> \n",
    "<p>A method used in natural language processing that combines the retrieval of relevant documents with a generative language model to enhance the model's ability to provide informative and accurate responses. This method is particularly useful for question-answering tasks and other applications where having access to a broad range of external information is beneficial.</p> \n",
    "<h3>Here's a breakdown of how RAG retrieval works</h3>\n",
    "<dl>\n",
    "    <dt>1. Retrieval Component</dt> \n",
    "    <dd>The first step in RAG retrieval involves retrieving relevant documents or information. When a query or question is posed, the system searches a large database of texts to find the most relevant documents. This database can be anything from a simple collection of articles to a comprehensive knowledge base.</dd>\n",
    "    <dt>2. Generative Component</dt> \n",
    "    <dd>Once the relevant documents are retrieved, a generative language model, like GPT (Generative Pretrained Transformer), is used. This model takes the input query and the retrieved documents as context to generate a response.</dd>\n",
    "    <dt>3. Combination of Retrieval and Generation</dt> \n",
    "    <dd>The key aspect of RAG retrieval is how it combines these two components. The retrieved documents provide the model with specific, detailed information relevant to the query, which the model might not have in its pre-trained knowledge. The generative model then synthesizes this information to create a coherent and contextually appropriate response.<dd>\n",
    "    <dt>3. Benefits<dt> \n",
    "    <dd>This approach allows the language model to answer questions or provide information that is more up-to-date, detailed, and specific than what it could generate based solely on its pre-trained knowledge. It's particularly useful for queries where the answer might not be common knowledge or is very specific.</dd>\n",
    "    <dt>4. Applications</dt> \n",
    "    <dd>RAG retrieval is commonly used in advanced chatbots, question-answering systems, and research tools. It's especially valuable in situations where keeping up with the latest information or covering a vast range of topics is essential.</dd>\n",
    "    <dt>Example:</dt> \n",
    "    <dd>Suppose someone asks a question about a recent scientific discovery. A RAG retrieval system would first find relevant scientific articles or papers about that discovery, then use a generative model to construct an answer that accurately reflects the current understanding as presented in those documents</dd>\n",
    "</dl>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"font-weight:bold\">Question and Answering vs. Fine-Tuning a Model</h2>\n",
    "<h3>Question and Answering (Q&A):</h3>\n",
    "<dl>\n",
    "    <dt>Definition:</dt> \n",
    "    <dd>Question and Answering in the context of AI and language models refers to the process where a model responds to queries posed in natural language. The goal is to provide accurate, relevant, and concise answers based on the model's training and knowledge.</dd>\n",
    "    <dt>Usage:<dt> \n",
    "    <dd>Q&A systems are widely used in chatbots, virtual assistants, and information retrieval systems. They are designed to understand a wide range of questions and provide answers that are drawn from their training data or real-time data sources.</dd>\n",
    "    <dt>Mechanism:</dt> \n",
    "    <dd>These systems typically use pre-trained models like GPT-3, which have been trained on vast amounts of text data, enabling them to generate responses based on patterns and information they have learned.</dd>\n",
    "<dl>\n",
    "<h3>Fine-Tuning a Model:</h3>\n",
    "<dl>\n",
    "    <dt>Definition:</dt> \n",
    "    <dd>Fine-tuning refers to the process of taking a pre-trained language model and further training it on a specific dataset to specialize its responses for a particular domain or task.</dd>\n",
    "    <dt>Usage:</dt> \n",
    "    <dd>Fine-tuning is common when you need a model to perform well on a specific type of data or task that was not the primary focus of the original, broader training. For instance, fine-tuning a model for medical Q&A, legal advice, or technical support.</dd>\n",
    "    <dt>Mechanism:</dt>\n",
    "    <dd>During fine-tuning, the model's weights are slightly adjusted so that it becomes more adept at understanding and generating responses relevant to the specialized domain. This process requires a smaller dataset and less computational power compared to training a model from scratch.</dd>\n",
    "</dl>\n",
    "<h3>Summary:</h3>\n",
    "<p>Q&A systems provide direct answers to user queries using pre-trained or fine-tuned models, while fine-tuning tailors a model to specific domains or tasks</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"font-weight: bold\">Advanced Retrieval vs. Basic Retrieval<h2>\n",
    "<h3>Basic Retrieval:</h3>\n",
    "<dl>\n",
    "    <dt>Definition:</dt> \n",
    "    <dd>Basic retrieval involves straightforward methods of finding information in response to a query. This often involves keyword matching, where the system looks for documents or data entries that contain the same words or phrases as the query.</dd>\n",
    "    <dt>Limitations:</dt> \n",
    "    <dd>Basic retrieval can be limited in handling complex queries, understanding context, or providing nuanced responses. It's generally less effective when dealing with ambiguous or multi-faceted questions.</dd>\n",
    "</dl>\n",
    "<h3>Advanced Retrieval:</h3>\n",
    "<dl>\n",
    "    <dt>Definition:</dt> \n",
    "    <dd>Advanced retrieval encompasses more sophisticated techniques that go beyond simple keyword matching. These methods might include semantic search, contextual understanding, and the integration of AI models.</dd>\n",
    "    <dt>Features:\n",
    "        <ul>\n",
    "            <dl>\n",
    "                <li><dt>Semantic Understanding:</dt></li> \n",
    "                <dd>Advanced systems understand the meaning behind words in a query, allowing them to retrieve information that is conceptually related, even if it doesn’t contain the exact keywords.</dd>\n",
    "                <li><dt>Context Awareness:</dt></li> \n",
    "                <dd>They can consider the context of a query, providing more relevant and precise results. For example, understanding the user's previous queries or the broader topic at hand.</dd>\n",
    "                <li><dt>Integration with AI:</dt></li> \n",
    "                <dd>Advanced retrieval often involves the use of AI models like neural networks, which can process and understand natural language at a more sophisticated level.</dd>\n",
    "            </dl>\n",
    "       </ul> \n",
    "    </dt>\n",
    "    <dt>Applications:</dt> \n",
    "    <dd>Advanced retrieval is essential in complex domains where queries require deep understanding and nuanced responses, such as in legal research, academic literature search, and specialized information databases.</dd>\n",
    "</dl>\n",
    "<h3>Summary:</h3>\n",
    "<p>Basic retrieval relies on simple keyword matching, whereas advanced retrieval uses more sophisticated techniques for a deeper understanding and contextual awareness in information retrieval</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"text_weight:bold\">Examples</h2>\n",
    "<h3 style=\"text_weight:bold\">LlamaIndex</h3>\n",
    "<p>LlamaIndex uses RAG Retrival. By using RAG Retrival LlamaIndex overcomes some weaknesses of the fine tuning approach:</p>\n",
    "<ul>\n",
    "    <li>There’s no training involved, so it’s cheap</li>\n",
    "    <li>Data is fetched only when you ask for them, so it’s always up to date</li>\n",
    "    <li>LlamaIndex can show you the retrieved documents, so it’s more trustworthy</li>\n",
    "</ul>\n",
    "<h4 style=\"text_weight:bold\">Tools Provided by LlamaIndex</h4>\n",
    "<dl>\n",
    "    <dt>Data Connectors</dt> \n",
    "    <dd>Ingest your existing data from their native source and format. These could be APIs, PDFs, SQL, and (much) more</dd>\n",
    "    <dt>Data Indexes</dt> \n",
    "    <dd>Structure your data in intermediate representations that are easy and performant for LLMs to consume</dd>\n",
    "    <dt>Engines provide natural language access to your data</dt>\n",
    "    <dd>For example: Query engines are powerful retrieval interfaces for knowledge-augmented output</dd>\n",
    "    <dd>For example: Chat engines are conversational interfaces for multi-message, “back and forth” interactions with your data</dd>\n",
    "    <dt>Data Agents</dt>\n",
    "    <dd>LLM-powered knowledge workers augmented by tools, from simple helper functions to API integrations and more</dd>\n",
    "    <dt>Application Integrations</dt>\n",
    "    <dd>Tie LlamaIndex back into the rest of your ecosystem. This could be LangChain, Flask, Docker, ChatGPT, or… anything else<dd>\n",
    "</dl>\n",
    "<h4>Documentation</h4>\n",
    "<p>To find out more about LlamaIndex and to see documentation please visit: <a href=https://docs.llamaindex.ai/en/stable>LlamaIndexDoc</a></p>\n",
    "<h4>Getting Started in LlamaIndex</h4>\n",
    "<p>Run this Line in the terminal to install LlamaIndex: pip install llama-index</p>\n",
    "<p>In the following example we are using LlamaIndex to find out information about the author in a currently written piece. Make sure the folder data is in the same directory of the python script or the Jupyter Notebook for this script to work. If the data file is not in the same directory make sure to update to file location</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting Imports and Uploading API Key. The API key should be stored in a the same directory and named .env for this script to work. If API key is stored elsewhere update code to find file location  \n",
    "from llama_index import (\n",
    "    VectorStoreIndex,\n",
    "    SimpleDirectoryReader,\n",
    "    StorageContext,\n",
    "    load_index_from_storage,\n",
    ")\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "# Load the environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "api_key=os.environ.get(\"OPENAI_API_KEY\"),\n",
    "\n",
    "if api_key is None:\n",
    "    raise ValueError(\"API_KEY is not set in the environment variables\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The author mentioned that before college, they worked on writing and programming. They wrote short stories and tried writing programs on the IBM 1401 computer. They also mentioned getting a microcomputer, a TRS-80, and started programming on it. They wrote simple games, a program to predict rocket heights, and even a word processor.\n"
     ]
    }
   ],
   "source": [
    "# check if storage already exists\n",
    "PERSIST_DIR = \"./storage\"\n",
    "if not os.path.exists(PERSIST_DIR):\n",
    "    # load the documents and create the index\n",
    "    documents = SimpleDirectoryReader(\"data\").load_data()\n",
    "    index = VectorStoreIndex.from_documents(documents)\n",
    "    # store it for later\n",
    "    index.storage_context.persist(persist_dir=PERSIST_DIR)\n",
    "else:\n",
    "    # load the existing index\n",
    "    storage_context = StorageContext.from_defaults(persist_dir=PERSIST_DIR)\n",
    "    index = load_index_from_storage(storage_context)\n",
    "\n",
    "# either way we can now query the index\n",
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(\"What did the author do growing up?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"font-weight=bold\">LangChain</h3>\n",
    "<p>LangChain is a framework for developing applications powered by language models. It enables applications that:\n",
    "<ul>\n",
    "<li>Are context-aware: connect a language model to sources of context (prompt instructions, few shot examples, content to ground its response in, etc.)</li>\n",
    "<li>Reason: rely on a language model to reason (about how to answer based on provided context, what actions to take, etc.)</li>\n",
    "</ul>\n",
    "This framework consists of LangChain Libraries, LangChain Templates, LangServe, and LangSmith.</p>\n",
    "<p>Together, these products simplify the entire application lifecycle:\n",
    "<dl>\n",
    "    <dt>Develop:</dt> \n",
    "    <dd>Write your applications in LangChain/LangChain.js. Hit the ground running using Templates for reference.</dd>\n",
    "    <dt>Productionize:</dt> \n",
    "    <dd>Use LangSmith to inspect, test and monitor your chains, so that you can constantly improve and deploy with confidence.</dd>\n",
    "    <dt>Deploy:</dt> \n",
    "    <dd>Turn any chain into an API with LangServe.</dd>\n",
    "</dl>\n",
    "For more information and documentaion of LangChain visit: <a href=https://python.langchain.com/docs/get_started/introduction>LangChain Doc</a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
