{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"text-align:center\">Garrett Zimmerman</h3>\n",
    "<h3 style=\"text-align:center\">January 6, 2024</h3>\n",
    "<h1 style=\"font-weight:bold;text-align:center\">Concepts of Rag  Retrieval</h1>\n",
    "<h2 style=\"font-weight:bold\">Goal:</h2>\n",
    "<p>\n",
    "<dl>\n",
    "    <dt style=\"font-weight:bold\">RAG Retrieval:</dt> \n",
    "    <dd>Review the concept of RAG Retrieval<dd> \n",
    "    <dd>Question and Answering vs. Fine Tuning a Model<dd>\n",
    "    <dd>Advanced  Retrieval vs. Basic Retrieval</dd>\n",
    "    <dt style=\"font-weight:bold\">Build Several Examples:</dt> \n",
    "    <dd>Explain different advance retrieval methods\n",
    "        <ul>\n",
    "            <li>LlamaIndex</li>\n",
    "            <li>LangChain</li>\n",
    "            <li>Pinecone</li>\n",
    "            <li>ChromaDB</li>\n",
    "        </ul>\n",
    "    </dd> \n",
    "</dl>\n",
    "</p> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"font-weight:bold\">RAG (Retrieval-Augmented Generation) Retrieval:</h2> \n",
    "<p>A method used in natural language processing that combines the retrieval of relevant documents with a generative language model to enhance the model's ability to provide informative and accurate responses. This method is particularly useful for question-answering tasks and other applications where having access to a broad range of external information is beneficial.</p> \n",
    "<h3>Here's a breakdown of how RAG retrieval works</h3>\n",
    "<dl>\n",
    "    <dt>1. Retrieval Component</dt> \n",
    "    <dd>The first step in RAG retrieval involves retrieving relevant documents or information. When a query or question is posed, the system searches a large database of texts to find the most relevant documents. This database can be anything from a simple collection of articles to a comprehensive knowledge base.</dd>\n",
    "    <dt>2. Generative Component</dt> \n",
    "    <dd>Once the relevant documents are retrieved, a generative language model, like GPT (Generative Pretrained Transformer), is used. This model takes the input query and the retrieved documents as context to generate a response.</dd>\n",
    "    <dt>3. Combination of Retrieval and Generation</dt> \n",
    "    <dd>The key aspect of RAG retrieval is how it combines these two components. The retrieved documents provide the model with specific, detailed information relevant to the query, which the model might not have in its pre-trained knowledge. The generative model then synthesizes this information to create a coherent and contextually appropriate response.<dd>\n",
    "    <dt>3. Benefits<dt> \n",
    "    <dd>This approach allows the language model to answer questions or provide information that is more up-to-date, detailed, and specific than what it could generate based solely on its pre-trained knowledge. It's particularly useful for queries where the answer might not be common knowledge or is very specific.</dd>\n",
    "    <dt>4. Applications</dt> \n",
    "    <dd>RAG retrieval is commonly used in advanced chatbots, question-answering systems, and research tools. It's especially valuable in situations where keeping up with the latest information or covering a vast range of topics is essential.</dd>\n",
    "    <dt>Example:</dt> \n",
    "    <dd>Suppose someone asks a question about a recent scientific discovery. A RAG retrieval system would first find relevant scientific articles or papers about that discovery, then use a generative model to construct an answer that accurately reflects the current understanding as presented in those documents</dd>\n",
    "</dl>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"font-weight:bold\">Question and Answering vs. Fine-Tuning a Model</h2>\n",
    "<h3>Question and Answering (Q&A):</h3>\n",
    "<dl>\n",
    "    <dt>Definition:</dt> \n",
    "    <dd>Question and Answering in the context of AI and language models refers to the process where a model responds to queries posed in natural language. The goal is to provide accurate, relevant, and concise answers based on the model's training and knowledge.</dd>\n",
    "    <dt>Usage:<dt> \n",
    "    <dd>Q&A systems are widely used in chatbots, virtual assistants, and information retrieval systems. They are designed to understand a wide range of questions and provide answers that are drawn from their training data or real-time data sources.</dd>\n",
    "    <dt>Mechanism:</dt> \n",
    "    <dd>These systems typically use pre-trained models like GPT-3, which have been trained on vast amounts of text data, enabling them to generate responses based on patterns and information they have learned.</dd>\n",
    "<dl>\n",
    "<h3>Fine-Tuning a Model:</h3>\n",
    "<dl>\n",
    "    <dt>Definition:</dt> \n",
    "    <dd>Fine-tuning refers to the process of taking a pre-trained language model and further training it on a specific dataset to specialize its responses for a particular domain or task.</dd>\n",
    "    <dt>Usage:</dt> \n",
    "    <dd>Fine-tuning is common when you need a model to perform well on a specific type of data or task that was not the primary focus of the original, broader training. For instance, fine-tuning a model for medical Q&A, legal advice, or technical support.</dd>\n",
    "    <dt>Mechanism:</dt>\n",
    "    <dd>During fine-tuning, the model's weights are slightly adjusted so that it becomes more adept at understanding and generating responses relevant to the specialized domain. This process requires a smaller dataset and less computational power compared to training a model from scratch.</dd>\n",
    "</dl>\n",
    "<h3>Summary:</h3>\n",
    "<p>Q&A systems provide direct answers to user queries using pre-trained or fine-tuned models, while fine-tuning tailors a model to specific domains or tasks</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"font-weight: bold\">Advanced Retrieval vs. Basic Retrieval<h2>\n",
    "<h3>Basic Retrieval:</h3>\n",
    "<dl>\n",
    "    <dt>Definition:</dt> \n",
    "    <dd>Basic retrieval involves straightforward methods of finding information in response to a query. This often involves keyword matching, where the system looks for documents or data entries that contain the same words or phrases as the query.</dd>\n",
    "    <dt>Limitations:</dt> \n",
    "    <dd>Basic retrieval can be limited in handling complex queries, understanding context, or providing nuanced responses. It's generally less effective when dealing with ambiguous or multi-faceted questions.</dd>\n",
    "</dl>\n",
    "<h3>Advanced Retrieval:</h3>\n",
    "<dl>\n",
    "    <dt>Definition:</dt> \n",
    "    <dd>Advanced retrieval encompasses more sophisticated techniques that go beyond simple keyword matching. These methods might include semantic search, contextual understanding, and the integration of AI models.</dd>\n",
    "    <dt>Features:\n",
    "        <ul>\n",
    "            <dl>\n",
    "                <li><dt>Semantic Understanding:</dt></li> \n",
    "                <dd>Advanced systems understand the meaning behind words in a query, allowing them to retrieve information that is conceptually related, even if it doesn’t contain the exact keywords.</dd>\n",
    "                <li><dt>Context Awareness:</dt></li> \n",
    "                <dd>They can consider the context of a query, providing more relevant and precise results. For example, understanding the user's previous queries or the broader topic at hand.</dd>\n",
    "                <li><dt>Integration with AI:</dt></li> \n",
    "                <dd>Advanced retrieval often involves the use of AI models like neural networks, which can process and understand natural language at a more sophisticated level.</dd>\n",
    "            </dl>\n",
    "       </ul> \n",
    "    </dt>\n",
    "    <dt>Applications:</dt> \n",
    "    <dd>Advanced retrieval is essential in complex domains where queries require deep understanding and nuanced responses, such as in legal research, academic literature search, and specialized information databases.</dd>\n",
    "</dl>\n",
    "<h3>Summary:</h3>\n",
    "<p>Basic retrieval relies on simple keyword matching, whereas advanced retrieval uses more sophisticated techniques for a deeper understanding and contextual awareness in information retrieval</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"font-weight:bold\">Examples</h2>\n",
    "<h3 style=\"font-weight:bold\">LlamaIndex</h3>\n",
    "<p>LlamaIndex uses RAG Retrival. By using RAG Retrival LlamaIndex overcomes some weaknesses of the fine tuning approach:</p>\n",
    "<ul>\n",
    "    <li>There’s no training involved, so it’s cheap</li>\n",
    "    <li>Data is fetched only when you ask for them, so it’s always up to date</li>\n",
    "    <li>LlamaIndex can show you the retrieved documents, so it’s more trustworthy</li>\n",
    "</ul>\n",
    "<h4 style=\"font-weight:bold\">Tools Provided by LlamaIndex</h4>\n",
    "<dl>\n",
    "    <dt>Data Connectors</dt> \n",
    "    <dd>Ingest your existing data from their native source and format. These could be APIs, PDFs, SQL, and (much) more</dd>\n",
    "    <dt>Data Indexes</dt> \n",
    "    <dd>Structure your data in intermediate representations that are easy and performant for LLMs to consume</dd>\n",
    "    <dt>Engines provide natural language access to your data</dt>\n",
    "    <dd>For example: Query engines are powerful retrieval interfaces for knowledge-augmented output</dd>\n",
    "    <dd>For example: Chat engines are conversational interfaces for multi-message, “back and forth” interactions with your data</dd>\n",
    "    <dt>Data Agents</dt>\n",
    "    <dd>LLM-powered knowledge workers augmented by tools, from simple helper functions to API integrations and more</dd>\n",
    "    <dt>Application Integrations</dt>\n",
    "    <dd>Tie LlamaIndex back into the rest of your ecosystem. This could be LangChain, Flask, Docker, ChatGPT, or… anything else<dd>\n",
    "</dl>\n",
    "<h4>Documentation</h4>\n",
    "<p>To find out more about LlamaIndex and to see documentation please visit: <a href=https://docs.llamaindex.ai/en/stable>LlamaIndexDoc</a></p>\n",
    "<h4>Getting Started in LlamaIndex</h4>\n",
    "<p>Run this Line in the terminal to install LlamaIndex: pip install llama-index</p>\n",
    "<p>In the following example we are using LlamaIndex to find out information about the author in a currently written piece. Make sure the folder data is in the same directory of the python script or the Jupyter Notebook for this script to work. If the data file is not in the same directory make sure to update to file location</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting Imports and Uploading API Key. The API key should be stored in a the same directory and named .env for this script to work. If API key is stored elsewhere update code to find file location  \n",
    "from llama_index import (\n",
    "    VectorStoreIndex,\n",
    "    SimpleDirectoryReader,\n",
    "    StorageContext,\n",
    "    load_index_from_storage,\n",
    ")\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "# Load the environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "#API Keys and Envriornments we will need in this notebook.\n",
    "required_env_vars = {\n",
    "    \"OPENAI_API_KEY\": \"OPENAI_API_KEY\",\n",
    "    \"PINECONE_OPEN_API_KEY\": \"PINECONE_OPEN_API_KEY\",\n",
    "    \"PINECONE_ENVIORNMENT\": \"PINECONE_ENVIORNMENT\"\n",
    "}\n",
    "# Iterate through the required environment variables\n",
    "for var, name in required_env_vars.items():\n",
    "    value = os.environ.get(var)\n",
    "    if value is None:\n",
    "        raise ValueError(f\"{name} is not set in the environment variables\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if storage already exists\n",
    "PERSIST_DIR = \"./storage\"\n",
    "if not os.path.exists(PERSIST_DIR):\n",
    "    # load the documents and create the index\n",
    "    documents = SimpleDirectoryReader(\"data\").load_data()\n",
    "    index = VectorStoreIndex.from_documents(documents)\n",
    "    # store it for later\n",
    "    index.storage_context.persist(persist_dir=PERSIST_DIR)\n",
    "else:\n",
    "    # load the existing index\n",
    "    storage_context = StorageContext.from_defaults(persist_dir=PERSIST_DIR)\n",
    "    index = load_index_from_storage(storage_context)\n",
    "\n",
    "# either way we can now query the index\n",
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(\"What did the author do growing up?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"font-weight:bold\">LangChain</h3>\n",
    "<p>LangChain is a framework for developing applications powered by language models. It enables applications that:\n",
    "<ul>\n",
    "<li>Are context-aware: connect a language model to sources of context (prompt instructions, few shot examples, content to ground its response in, etc.)</li>\n",
    "<li>Reason: rely on a language model to reason (about how to answer based on provided context, what actions to take, etc.)</li>\n",
    "</ul>\n",
    "This framework consists of LangChain Libraries, LangChain Templates, LangServe, and LangSmith.</p>\n",
    "<p>Together, these products simplify the entire application lifecycle:\n",
    "<dl>\n",
    "    <dt>Develop:</dt> \n",
    "    <dd>Write your applications in LangChain/LangChain.js. Hit the ground running using Templates for reference.</dd>\n",
    "    <dt>Productionize:</dt> \n",
    "    <dd>Use LangSmith to inspect, test and monitor your chains, so that you can constantly improve and deploy with confidence.</dd>\n",
    "    <dt>Deploy:</dt> \n",
    "    <dd>Turn any chain into an API with LangServe.</dd>\n",
    "</dl>\n",
    "For more information and documentaion of LangChain visit: <a href=https://python.langchain.com/docs/get_started/introduction>LangChain Doc</a></p>\n",
    "<p>To start make sure to install LangChain using : pip install langchain</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>LangChain Steps</h2>\n",
    "<dl>\n",
    "    <dt>1. Document Loaders</dt>\n",
    "    <dd>Use document loaders to load data from a source as Document's. A Document is a piece of text and associated metadata. For example, there are document loaders for loading a simple .txt file, for loading the text contents of any web page, or even for loading a transcript of a YouTube video. Document loaders provide a \"load\" method for loading data as documents from a configured source. They optionally implement a \"lazy load\" as well for lazily loading data into memory.</dd>\n",
    "    <dt>2. Text Splitters</dt>\n",
    "    <dd>Once you've loaded documents, you'll often want to transform them to better suit your application. The simplest example is you may want to split a long document into smaller chunks that can fit into your model's context window. LangChain has a number of built-in document transformers that make it easy to split, combine, filter, and otherwise manipulate documents. When you want to deal with long pieces of text, it is necessary to split up that text into chunks. As simple as this sounds, there is a lot of potential complexity here. Ideally, you want to keep the semantically related pieces of text together. What \"semantically related\" means could depend on the type of text. At a high level, text splitters work as following:\n",
    "        <ol>\n",
    "        <li> Split the text up into small, semantically meaningful chunks (often sentences).</li>\n",
    "        <li>Start combining these small chunks into a larger chunk until you reach a certain size (as measured by some function).</li>\n",
    "        <li>Once you reach that size, make that chunk its own piece of text and then start creating a new chunk of text with some overlap (to keep context between chunks).</li>\n",
    "        </ol>\n",
    "        That means there are two different axes along which you can customize your text splitter:\n",
    "        <ol>\n",
    "        <li>How the text is split</li>\n",
    "        <li>How the chunk size is measured</li>\n",
    "        </ol>\n",
    "</dl>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "#pip3 install beautifulsoup4\n",
    "#Step 1 Load the document\n",
    "with open(\"D:\\Rag Retrival Zimm\\data\\What I Worked On.txt\", 'r') as file:\n",
    "    document_text = file.read()\n",
    "#Step 1.5 Cleaning the Document of HTML\n",
    "soup = BeautifulSoup(document_text, 'html.parser')\n",
    "clean_text = soup.get_text()\n",
    "# Remove newline characters\n",
    "clean_text = clean_text.replace('\\n', ' ')  # Replace with a space or '' to remove entirely\n",
    "# Replace the escaped single quote\n",
    "clean_text = clean_text.replace(\"\\\\'\", \"'\")\n",
    "display(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Pinecone</h2>\n",
    "<p>Pinecone makes it easy to provide long-term memory for high-performance AI applications. It’s a managed, cloud-native vector database with a simple API and no infrastructure hassles. Pinecone serves fresh, filtered query results with low latency at the scale of billions of vectors.</p>\n",
    "<ul>\n",
    "<li>Vector embeddings provide long-term memory for AI</li>\n",
    "<li>Vector databases store and query embeddings quickly and at scale</li>\n",
    "</ul>\n",
    "<h3>Install Pinecone: pip install pinecone-client </h3>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"font-weight:bold\">Creating an Index</h3>\n",
    "<h3>Distance metrics</h3>\n",
    "<p>You can choose from different metrics when creating a vector index:</p>\n",
    "<dl>\n",
    "    <dt>euclidean</dt>\n",
    "    <dd><ul>\n",
    "    <li>This is used to calculate the distance between two data points in a plane. It is one of the most commonly used distance metric</li>\n",
    "    <li>When you use metric='euclidean', the most similar results are those with the lowest score</li></ul></dd>\n",
    "    <dt>cosine</dt>\n",
    "    <dd><ul>\n",
    "    <li>This is often used to find similarities between different documents. The advantage is that the scores are normalized to [-1,1] range</li></ul></dd>\n",
    "    <dt>dotproduct</dt>\n",
    "    <dd><ul>\n",
    "    <li>This is used to multiply two vectors. You can use it to tell us how similar the two vectors are. The more positive the answer is, the closer the two vectors are in terms of their directions</li></ul></dd>\n",
    "</dl>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pincone-example-index']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pinecone\n",
    "from tqdm.autonotebook import tqdm\n",
    "\n",
    "pinecone.init(api_key=pincone_api_key, environment=pincone_env)\n",
    "#Creating an Index\n",
    "index_name=\"pincone-example-index\"\n",
    "#delete the index if an index of the same name already exists\n",
    "if index_name in pinecone.list_indexes():\n",
    "    pinecone.delete_index(index_name)\n",
    "#Create Index\n",
    "my_dimensions = 3\n",
    "#distance metric\n",
    "distance_metric = \"cosine\"\n",
    "#shards\n",
    "shards_used=1\n",
    "pinecone.create_index(name=index_name, dimension=my_dimensions, metric=distance_metric, shards=shards_used)\n",
    "#connect to index\n",
    "index=pinecone.Index(index_name=index_name)\n",
    "#Display active Indexes\n",
    "pinecone.list_indexes()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
