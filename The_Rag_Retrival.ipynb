{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"text-align:center\">Garrett Zimmerman</h3>\n",
    "<h3 style=\"text-align:center\">January 6, 2024</h3>\n",
    "<h1 style=\"font-weight:bold;text-align:center\">Concepts of Rag  Retrieval</h1>\n",
    "<h2 style=\"font-weight:bold\">Goal:</h2>\n",
    "<p>\n",
    "<dl>\n",
    "    <dt style=\"font-weight:bold\">RAG Retrieval:</dt> \n",
    "    <dd>Review the concept of RAG Retrieval<dd> \n",
    "    <dd>Question and Answering vs. Fine Tuning a Model<dd>\n",
    "    <dd>Advanced  Retrieval vs. Basic Retrieval</dd>\n",
    "    <dt style=\"font-weight:bold\">Things to consider before using RAG Retrieval</dt>\n",
    "    <dd>Understanding Chunking in NLP</dd>\n",
    "    <dd>Developing an Optimal Chunking Strategy<dd>\n",
    "    <dd>Understanding and Utilizing Embeddings</dd>\n",
    "    <dd>Utilizing Metadata in Data Processing</dd>\n",
    "    <dt style=\"font-weight:bold\">Build Several Examples:</dt> \n",
    "    <dd>Explain different advance retrieval methods\n",
    "        <ul>\n",
    "            <li>LlamaIndex</li>\n",
    "            <li>LangChain</li>\n",
    "            <li>Pinecone</li>\n",
    "            <li>ChromaDB</li>\n",
    "        </ul>\n",
    "    </dd> \n",
    "</dl>\n",
    "</p> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"font-weight:bold\">RAG (Retrieval-Augmented Generation) Retrieval:</h2> \n",
    "<p>A method used in natural language processing that combines the retrieval of relevant documents with a generative language model to enhance the model's ability to provide informative and accurate responses. This method is particularly useful for question-answering tasks and other applications where having access to a broad range of external information is beneficial.</p> \n",
    "<h3>Here's a breakdown of how RAG retrieval works</h3>\n",
    "<dl>\n",
    "    <dt>1. Retrieval Component</dt> \n",
    "    <dd>The first step in RAG retrieval involves retrieving relevant documents or information. When a query or question is posed, the system searches a large database of texts to find the most relevant documents. This database can be anything from a simple collection of articles to a comprehensive knowledge base.</dd>\n",
    "    <dt>2. Generative Component</dt> \n",
    "    <dd>Once the relevant documents are retrieved, a generative language model, like GPT (Generative Pretrained Transformer), is used. This model takes the input query and the retrieved documents as context to generate a response.</dd>\n",
    "    <dt>3. Combination of Retrieval and Generation</dt> \n",
    "    <dd>The key aspect of RAG retrieval is how it combines these two components. The retrieved documents provide the model with specific, detailed information relevant to the query, which the model might not have in its pre-trained knowledge. The generative model then synthesizes this information to create a coherent and contextually appropriate response.<dd>\n",
    "    <dt>3. Benefits<dt> \n",
    "    <dd>This approach allows the language model to answer questions or provide information that is more up-to-date, detailed, and specific than what it could generate based solely on its pre-trained knowledge. It's particularly useful for queries where the answer might not be common knowledge or is very specific.</dd>\n",
    "    <dt>4. Applications</dt> \n",
    "    <dd>RAG retrieval is commonly used in advanced chatbots, question-answering systems, and research tools. It's especially valuable in situations where keeping up with the latest information or covering a vast range of topics is essential.</dd>\n",
    "    <dt>Example:</dt> \n",
    "    <dd>Suppose someone asks a question about a recent scientific discovery. A RAG retrieval system would first find relevant scientific articles or papers about that discovery, then use a generative model to construct an answer that accurately reflects the current understanding as presented in those documents</dd>\n",
    "</dl>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"font-weight:bold\">Question and Answering vs. Fine-Tuning a Model</h2>\n",
    "<h3>Question and Answering (Q&A):</h3>\n",
    "<dl>\n",
    "    <dt>Definition:</dt> \n",
    "    <dd>Question and Answering in the context of AI and language models refers to the process where a model responds to queries posed in natural language. The goal is to provide accurate, relevant, and concise answers based on the model's training and knowledge.</dd>\n",
    "    <dt>Usage:<dt> \n",
    "    <dd>Q&A systems are widely used in chatbots, virtual assistants, and information retrieval systems. They are designed to understand a wide range of questions and provide answers that are drawn from their training data or real-time data sources.</dd>\n",
    "    <dt>Mechanism:</dt> \n",
    "    <dd>These systems typically use pre-trained models like GPT-3, which have been trained on vast amounts of text data, enabling them to generate responses based on patterns and information they have learned.</dd>\n",
    "<dl>\n",
    "<h3>Fine-Tuning a Model:</h3>\n",
    "<dl>\n",
    "    <dt>Definition:</dt> \n",
    "    <dd>Fine-tuning refers to the process of taking a pre-trained language model and further training it on a specific dataset to specialize its responses for a particular domain or task.</dd>\n",
    "    <dt>Usage:</dt> \n",
    "    <dd>Fine-tuning is common when you need a model to perform well on a specific type of data or task that was not the primary focus of the original, broader training. For instance, fine-tuning a model for medical Q&A, legal advice, or technical support.</dd>\n",
    "    <dt>Mechanism:</dt>\n",
    "    <dd>During fine-tuning, the model's weights are slightly adjusted so that it becomes more adept at understanding and generating responses relevant to the specialized domain. This process requires a smaller dataset and less computational power compared to training a model from scratch.</dd>\n",
    "</dl>\n",
    "<h3>Summary:</h3>\n",
    "<p>Q&A systems provide direct answers to user queries using pre-trained or fine-tuned models, while fine-tuning tailors a model to specific domains or tasks</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"font-weight: bold\">Advanced Retrieval vs. Basic Retrieval<h2>\n",
    "<h3>Basic Retrieval:</h3>\n",
    "<dl>\n",
    "    <dt>Definition:</dt> \n",
    "    <dd>Basic retrieval involves straightforward methods of finding information in response to a query. This often involves keyword matching, where the system looks for documents or data entries that contain the same words or phrases as the query.</dd>\n",
    "    <dt>Limitations:</dt> \n",
    "    <dd>Basic retrieval can be limited in handling complex queries, understanding context, or providing nuanced responses. It's generally less effective when dealing with ambiguous or multi-faceted questions.</dd>\n",
    "</dl>\n",
    "<h3>Advanced Retrieval:</h3>\n",
    "<dl>\n",
    "    <dt>Definition:</dt> \n",
    "    <dd>Advanced retrieval encompasses more sophisticated techniques that go beyond simple keyword matching. These methods might include semantic search, contextual understanding, and the integration of AI models.</dd>\n",
    "    <dt>Features:\n",
    "        <ul>\n",
    "            <dl>\n",
    "                <li><dt>Semantic Understanding:</dt></li> \n",
    "                <dd>Advanced systems understand the meaning behind words in a query, allowing them to retrieve information that is conceptually related, even if it doesn’t contain the exact keywords.</dd>\n",
    "                <li><dt>Context Awareness:</dt></li> \n",
    "                <dd>They can consider the context of a query, providing more relevant and precise results. For example, understanding the user's previous queries or the broader topic at hand.</dd>\n",
    "                <li><dt>Integration with AI:</dt></li> \n",
    "                <dd>Advanced retrieval often involves the use of AI models like neural networks, which can process and understand natural language at a more sophisticated level.</dd>\n",
    "            </dl>\n",
    "       </ul> \n",
    "    </dt>\n",
    "    <dt>Applications:</dt> \n",
    "    <dd>Advanced retrieval is essential in complex domains where queries require deep understanding and nuanced responses, such as in legal research, academic literature search, and specialized information databases.</dd>\n",
    "</dl>\n",
    "<h3>Summary:</h3>\n",
    "<p>Basic retrieval relies on simple keyword matching, whereas advanced retrieval uses more sophisticated techniques for a deeper understanding and contextual awareness in information retrieval</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Questions to cosider before</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Understanding Chunking in NLP</h2>\n",
    "<p>Chunking is a crucial technique in Natural Language Processing (NLP) that involves segmenting and categorizing parts of a sentence into shorter, meaningful phrases or \"chunks.\" This section provides insights into the concept of chunking, its utility, various methods, and the inherent ambiguity it might involve.</p>\n",
    "\n",
    "<h3>What is Chunking?</h3>\n",
    "<p>Chunking refers to the process of breaking down sentences into smaller, semantically significant phrases known as 'chunks.' These chunks can be noun phrases, verb phrases, or any meaningful grouping of words. The main objective of chunking is to add more structure to sentences, making them easier to analyze and understand.</p>\n",
    "\n",
    "<h3>Why is Chunking Useful?</h3>\n",
    "<ul>\n",
    "    <li><strong>Structural Insights:</strong> Chunking helps in revealing the structure of sentences, making it easier to understand their construction and meaning.</li>\n",
    "    <li><strong>Simplifies Processing:</strong> It breaks down complex sentences into manageable pieces, simplifying further linguistic processing.</li>\n",
    "    <li><strong>Improves Performance:</strong> Chunking is particularly useful in tasks like named entity recognition and information extraction, where understanding the context is crucial.</li>\n",
    "    <li><strong>Enhances Understanding:</strong> It aids in identifying relationships between different parts of a sentence, thereby enhancing the overall comprehension of the text.</li>\n",
    "</ul>\n",
    "\n",
    "<h3>Different Chunking Methods</h3>\n",
    "<ul>\n",
    "    <li><strong>Rule-Based Chunking:</strong> Utilizes a set of predefined rules to identify chunks. Effective but can be rigid.</li>\n",
    "    <li><strong>Regular Expression-Based Chunking:</strong> Employs regular expressions to find patterns in text, offering flexibility in identifying chunks.</li>\n",
    "    <li><strong>Supervised Learning Chunking:</strong> Involves training a model on a labeled dataset to learn chunking, requiring a significant amount of labeled data but can be highly effective.</li>\n",
    "    <li><strong>Unsupervised Learning Chunking:</strong> Uses techniques like clustering to identify chunks based on the statistical properties of the text, without relying on labeled data.</li>\n",
    "</ul>\n",
    "\n",
    "<h3>Why is Chunking Ambiguous?</h3>\n",
    "<p>Despite its usefulness, chunking is not always straightforward due to the complexities and nuances of human language:</p>\n",
    "<ul>\n",
    "    <li><strong>Linguistic Ambiguity:</strong> The same set of words can be chunked differently depending on the context, leading to multiple valid interpretations.</li>\n",
    "    <li><strong>Overlap of Chunks:</strong> Some words may fit into multiple chunks, creating ambiguity in their categorization.</li>\n",
    "    <li><strong>Complex Rules:</strong> The intricacy of rules in rule-based chunking can lead to situations where it's unclear which rule should apply, especially in complex sentences.</li>\n",
    "    <li><strong>Variability in Language:</strong> The presence of slang, idioms, or domain-specific terms can add another layer of complexity to the chunking process.</li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Developing an Optimal Chunking Strategy</h2>\n",
    "<p>Creating an optimal chunking strategy involves a systematic approach that considers the nature of the data, the requirements of the application, and iterative testing and refinement. Below are the steps and considerations to guide you in developing an optimal chunking strategy through code:</p>\n",
    "\n",
    "<h3>1. Define the Objective</h3>\n",
    "<p>Clearly understand what you need from chunking. The objective significantly influences how you should approach chunking your text.</p>\n",
    "\n",
    "<h3>2. Understand Your Data</h3>\n",
    "<p>Analyze the structure and nature of your data. Different types of text may necessitate different chunking strategies.</p>\n",
    "\n",
    "<h3>3. Choose a Preliminary Chunking Method</h3>\n",
    "<p>Select an initial chunking method based on your objective and data type. This could range from simple rule-based methods to more complex machine learning-based approaches.</p>\n",
    "\n",
    "<h3>4. Develop a Scoring System</h3>\n",
    "<p>Create a system to evaluate the effectiveness of your chunking. This could involve metrics like precision, recall, or the impact of chunking on downstream tasks.</p>\n",
    "\n",
    "<h3>5. Iteratively Test and Refine</h3>\n",
    "<p>Test your chunking on a sample of your data, evaluate it using your scoring system, and refine your approach based on the results.</p>\n",
    "\n",
    "<h3>6. Leverage Existing Tools and Libraries</h3>\n",
    "<p>Consider tools like Llama Index, Haystack, Semantic Kernel, or LLMware, which might provide insights, tools, or methodologies for effective chunking.</p>\n",
    "\n",
    "<h3>7. Continuous Improvement</h3>\n",
    "<p>Refine your chunking strategy based on feedback and performance on your specific tasks. Adapt the strategy as your dataset grows or changes, or as your application evolves.</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Understanding and Utilizing Embeddings</h2>\n",
    "<p>Embeddings are a fundamental concept in machine learning and NLP for converting high-dimensional data like text or images into a lower-dimensional, dense vector representation. This section explores embeddings, their uses, and considerations for using low-dimensional versus high-dimensional models.</p>\n",
    "\n",
    "<h3>What are Embeddings?</h3>\n",
    "<p>Embeddings transform words, phrases, or entire documents into vectors of real numbers, capturing the semantic meaning of the text in a lower-dimensional space.</p>\n",
    "\n",
    "<h3>How are Embeddings Used?</h3>\n",
    "<ul>\n",
    "    <li>For measuring similarity between texts, classifying text, retrieving information, translating languages, and understanding natural language.</li>\n",
    "</ul>\n",
    "\n",
    "<h3>Low-Dimensional vs. High-Dimensional Models</h3>\n",
    "<p>Choose low-dimensional models for efficiency and high-dimensional models for capturing nuanced semantic details. The choice depends on the specific requirements of your task and the computational resources available.</p>\n",
    "\n",
    "<h3>Small to Big / Big to Small Chunk to Embedding Mapping</h3>\n",
    "<p>Consider the granularity required for your task. Use fine-grained (small to big) approaches for detail-oriented tasks and coarse-grained (big to small) approaches for broader understanding or summarization tasks.</p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Utilizing Metadata in Data Processing</h2>\n",
    "<p>Metadata provides valuable context and additional information about primary data sources. This section discusses the use of metadata, its benefits, and the potential of leveraging Large Language Models (LLMs) to generate metadata for chunks of text.</p>\n",
    "\n",
    "<h3>How to Use Metadata</h3>\n",
    "<p>Metadata is used for organizing, interpreting, processing, and integrating data. It enhances the manageability and usability of data in various applications.</p>\n",
    "\n",
    "<h3>Why Use Metadata</h3>\n",
    "<ul>\n",
    "    <li>Metadata enhances searchability, data management, contextual understanding, and compliance with regulatory standards.</li>\n",
    "</ul>\n",
    "\n",
    "<h3>Using LLMs to Generate Metadata for Chunks</h3>\n",
    "<p>LLMs can be employed to automatically generate metadata for text chunks, such as automatic tag generation, summarization, sentiment analysis, and named entity recognition.</p>\n",
    "\n",
    "<h3>Implementation Considerations</h3>\n",
    "<ul>\n",
    "    <li>Ensure the quality and relevance of automatically generated metadata, consider scalability, and maintain privacy and security, especially when using LLMs for metadata generation.</li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"font-weight:bold\">Examples</h2>\n",
    "<h3 style=\"font-weight:bold\">LlamaIndex</h3>\n",
    "<p>LlamaIndex uses RAG Retrival. By using RAG Retrival LlamaIndex overcomes some weaknesses of the fine tuning approach:</p>\n",
    "<ul>\n",
    "    <li>There’s no training involved, so it’s cheap</li>\n",
    "    <li>Data is fetched only when you ask for them, so it’s always up to date</li>\n",
    "    <li>LlamaIndex can show you the retrieved documents, so it’s more trustworthy</li>\n",
    "</ul>\n",
    "<h4 style=\"font-weight:bold\">Tools Provided by LlamaIndex</h4>\n",
    "<dl>\n",
    "    <dt>Data Connectors</dt> \n",
    "    <dd>Ingest your existing data from their native source and format. These could be APIs, PDFs, SQL, and (much) more</dd>\n",
    "    <dt>Data Indexes</dt> \n",
    "    <dd>Structure your data in intermediate representations that are easy and performant for LLMs to consume</dd>\n",
    "    <dt>Engines provide natural language access to your data</dt>\n",
    "    <dd>For example: Query engines are powerful retrieval interfaces for knowledge-augmented output</dd>\n",
    "    <dd>For example: Chat engines are conversational interfaces for multi-message, “back and forth” interactions with your data</dd>\n",
    "    <dt>Data Agents</dt>\n",
    "    <dd>LLM-powered knowledge workers augmented by tools, from simple helper functions to API integrations and more</dd>\n",
    "    <dt>Application Integrations</dt>\n",
    "    <dd>Tie LlamaIndex back into the rest of your ecosystem. This could be LangChain, Flask, Docker, ChatGPT, or… anything else<dd>\n",
    "</dl>\n",
    "<h4>Documentation</h4>\n",
    "<p>To find out more about LlamaIndex and to see documentation please visit: <a href=https://docs.llamaindex.ai/en/stable>LlamaIndexDoc</a></p>\n",
    "<h4>Getting Started in LlamaIndex</h4>\n",
    "<p>Run this Line in the terminal to install LlamaIndex: pip install llama-index</p>\n",
    "<p>In the following example we are using LlamaIndex to find out information about the author in a currently written piece. Make sure the folder data is in the same directory of the python script or the Jupyter Notebook for this script to work. If the data file is not in the same directory make sure to update to file location</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting Imports and Uploading API Key. The API key should be stored in a the same directory and named .env for this script to work. If API key is stored elsewhere update code to find file location  \n",
    "import nltk\n",
    "from llama_index import (\n",
    "    VectorStoreIndex,\n",
    "    SimpleDirectoryReader,\n",
    "    StorageContext,\n",
    "    load_index_from_storage,\n",
    ")\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "# Load the environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "#API Keys and Envriornments we will need in this notebook.\n",
    "required_env_vars = {\n",
    "    \"OPENAI_API_KEY\": \"OPENAI_API_KEY\",\n",
    "    \"PINECONE_OPEN_API_KEY\": \"PINECONE_OPEN_API_KEY\",\n",
    "    \"PINECONE_ENVIORNMENT\": \"PINECONE_ENVIORNMENT\"\n",
    "}\n",
    "# Iterate through the required environment variables\n",
    "for var, name in required_env_vars.items():\n",
    "    value = os.environ.get(var)\n",
    "    if value is None:\n",
    "        raise ValueError(f\"{name} is not set in the environment variables\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The author mentioned that before college, they worked on writing and programming. They wrote short stories and tried writing programs on the IBM 1401 computer. They also mentioned getting a microcomputer, a TRS-80, and started programming on it. They wrote simple games, a program to predict rocket heights, and even a word processor.\n"
     ]
    }
   ],
   "source": [
    "# check if storage already exists\n",
    "PERSIST_DIR = \"./storage\"\n",
    "if not os.path.exists(PERSIST_DIR):\n",
    "    # load the documents and create the index\n",
    "    documents = SimpleDirectoryReader(\"data\").load_data()\n",
    "    index = VectorStoreIndex.from_documents(documents)\n",
    "    # store it for later\n",
    "    index.storage_context.persist(persist_dir=PERSIST_DIR)\n",
    "else:\n",
    "    # load the existing index\n",
    "    storage_context = StorageContext.from_defaults(persist_dir=PERSIST_DIR)\n",
    "    index = load_index_from_storage(storage_context)\n",
    "\n",
    "# either way we can now query the index\n",
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(\"What did the author do growing up?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"font-weight:bold\">Understanding LangChain</h3>\n",
    "<p>LangChain is an advanced framework designed for building applications that leverage the capabilities of language models. This framework is instrumental in creating applications that are both context-aware and capable of complex reasoning. Here's a deeper look into the core features and the application lifecycle within the LangChain framework:</p>\n",
    "\n",
    "<ul>\n",
    "    <li><strong>Context-awareness:</strong> LangChain excels in connecting language models to various sources of context. This includes prompt instructions, few-shot examples, and specific content that the language model can use as a grounding for its responses. This ability ensures that the application's responses are relevant and informed by the appropriate context.</li>\n",
    "    <li><strong>Reasoning:</strong> The framework utilizes the language model's capability to reason. This involves determining appropriate responses based on the provided context or deciding on actions to take in a given scenario. LangChain's reasoning capabilities are central to creating applications that can handle complex tasks and provide insightful outputs.</li>\n",
    "</ul>\n",
    "\n",
    "<p>The LangChain ecosystem simplifies the entire application lifecycle, encompassing development, production, and deployment:</p>\n",
    "\n",
    "<dl>\n",
    "    <dt><strong>Develop:</strong></dt> \n",
    "    <dd>LangChain facilitates the application development process. Utilize LangChain or LangChain.js to craft your applications efficiently. Benefit from the ready-to-use Templates, which provide a solid foundation and reference for your projects.</dd>\n",
    "    <dt><strong>Productionize:</strong></dt> \n",
    "    <dd>LangChain ensures that your applications are robust and reliable. With LangSmith, you can inspect, test, and monitor your chains, allowing for continuous improvement. This tool ensures that you can deploy your applications with confidence, knowing they have been thoroughly vetted and optimized.</dd>\n",
    "    <dt><strong>Deploy:</strong></dt> \n",
    "    <dd>LangChain simplifies the deployment process. With LangServe, you can transform any chain into an API, making it easy to integrate your application into various platforms and services. This feature streamlines the deployment process, allowing for seamless integration and scalability.</dd>\n",
    "</dl>\n",
    "\n",
    "<p>LangChain is a versatile framework that caters to a wide range of applications, from simple utilities to complex systems requiring nuanced understanding and decision-making capabilities. For comprehensive information, guidance, and documentation on leveraging LangChain for your projects, visit the official <a href=\"https://python.langchain.com/docs/get_started/introduction\">LangChain Documentation</a>.</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Using LangChain with FAISS for Document Retrieval</h2>\n",
    "<p>This section demonstrates how to use LangChain and FAISS to efficiently process, embed, and query text documents. The process involves loading documents, splitting them into manageable chunks, embedding these chunks for semantic search, and querying the embedded documents for relevant information.</p>\n",
    "<dl>\n",
    "    <dt><strong>1. Installation</strong></dt>\n",
    "    <dd>Install the necessary packages using pip. LangChain provides the functionalities for processing and embedding the text, while FAISS offers an efficient similarity search for the embedded vectors.</dd>\n",
    "    <dd><code>pip install langchain-openai</code></dd>\n",
    "    <dd><code>pip install faiss-cpu</code></dd>\n",
    "    <dt><strong>2. Load Documents</strong></dt>\n",
    "    <dd>Documents are loaded from a specified path. The TextLoader utility from LangChain reads the text data, preparing it for further processing and embedding.</dd>   \n",
    "    <dt><strong>3. Chunk the Documents</strong></dt>\n",
    "    <dd>To manage large documents and improve processing efficiency, the text is split into smaller chunks. The CharacterTextSplitter utility is used here to divide the text based on character count, ensuring each chunk is of a manageable size for embedding.</dd>   \n",
    "    <dt><strong>4. Embed and Store the Documents</strong></dt>\n",
    "    <dd>Each text chunk is embedded using the OpenAIEmbeddings, which converts the text into high-dimensional vectors capturing the semantic meaning of the text. These vectors are then stored in a FAISS vector store, an efficient structure for similarity search and retrieval tasks.</dd>   \n",
    "    <dt><strong>5. Query the Vector Store</strong></dt>\n",
    "    <dd>The FAISS vector store allows for querying the embedded documents. By inputting a query, the store returns the most semantically similar document chunks. This process leverages the power of semantic embeddings to retrieve relevant information from the text corpus based on the query context.</dd>\n",
    "</dl>\n",
    "<p>Through these steps, LangChain and FAISS together provide a robust framework for processing, embedding, and semantically searching large text corpora, making it an excellent tool for information retrieval and knowledge extraction.</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 1004, which is longer than the specified 1000\n",
      "Created a chunk of size 1203, which is longer than the specified 1000\n",
      "Created a chunk of size 1025, which is longer than the specified 1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What I Worked On\n",
      "\n",
      "February 2021\n",
      "\n",
      "Before college the two main things I worked on, outside of school, were writing and programming. I didn't write essays. I wrote what beginning writers were supposed to write then, and probably still are: short stories. My stories were awful. They had hardly any plot, just characters with strong feelings, which I imagined made them deep.\n",
      "\n",
      "The first programs I tried writing were on the IBM 1401 that our school district used for what was then called \"data processing.\" This was in 9th grade, so I was 13 or 14. The school district's 1401 happened to be in the basement of our junior high school, and my friend Rich Draves and I got permission to use it. It was like a mini Bond villain's lair down there, with all these alien-looking machines â€” CPU, disk drives, printer, card reader â€” sitting up on a raised floor under bright fluorescent lights.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "#2: Load the document  embed each chunk and load it into the vector store.\n",
    "raw_documents = TextLoader(\"D:\\Rag Retrival Zimm\\data\\What I Worked On.txt\").load()\n",
    "#3: Split it into chunks\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "documents = text_splitter.split_documents(raw_documents)\n",
    "#4: Embed each chunk and load it into the vector store\n",
    "db = FAISS.from_documents(documents, OpenAIEmbeddings())\n",
    "\n",
    "#query The Document\n",
    "query = \"What did the author do growing up?\"\n",
    "docs = db.similarity_search(query)\n",
    "print(docs[0].page_content)\n",
    "context_LangChain=docs[0].page_content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<p>The pertinent excerpts of text related to our query have been meticulously extracted from the larger document. By inputting these specific segments into a Large Language Model (LLM), we are now poised to generate a comprehensive and coherent response, effectively harnessing the advanced capabilities of AI-driven natural language processing.</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The author focused on two main activities while growing up: writing and programming. They wrote short stories, although they admit that their stories were not very good. In terms of programming, they started experimenting with writing programs on the IBM 1401 computer that their school district used for data processing. They were granted permission to use the computer, which was located in the basement of their junior high school.\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "prompt = f\"Context: {context_LangChain}\\nQuestion: What did the author do growing up?\\nAnswer:\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are helping someone answer the question\"},\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "  ]\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Pinecone</h2>\n",
    "<p>Pinecone makes it easy to provide long-term memory for high-performance AI applications. It’s a managed, cloud-native vector database with a simple API and no infrastructure hassles. Pinecone serves fresh, filtered query results with low latency at the scale of billions of vectors.</p>\n",
    "<ul>\n",
    "<li>Vector embeddings provide long-term memory for AI</li>\n",
    "<li>Vector databases store and query embeddings quickly and at scale</li>\n",
    "</ul>\n",
    "<h3>Install Pinecone: pip install pinecone-client </h3>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"font-weight:bold\">Creating an Index</h3>\n",
    "<p>Think of a Pinecone Index as a database that stores and organizes our information for efficient retrieval.</p>\n",
    "\n",
    "<h3>Distance Metrics</h3>\n",
    "<p>When creating a vector index in Pinecone, you can choose from different distance metrics, each with its unique characteristics:</p>\n",
    "\n",
    "<dl>\n",
    "    <dt><strong>Euclidean</strong></dt>\n",
    "    <dd>\n",
    "        <ul>\n",
    "            <li>Used to calculate the straight-line distance between two points. It's a common choice for spatial data.</li>\n",
    "            <li>With 'metric=euclidean', closer vectors have lower scores, indicating higher similarity.</li>\n",
    "        </ul>\n",
    "    </dd>\n",
    "    <dt><strong>Cosine</strong></dt>\n",
    "    <dd>\n",
    "        <ul>\n",
    "            <li>Commonly used for text data, measuring the cosine of the angle between vectors. It normalizes scores to a [-1, 1] range.</li>\n",
    "        </ul>\n",
    "    </dd>\n",
    "    <dt><strong>Dot Product</strong></dt>\n",
    "    <dd>\n",
    "        <ul>\n",
    "            <li>Calculates the product of two vectors, indicating how similar they are in direction. Higher positive scores denote greater similarity.</li>\n",
    "        </ul>\n",
    "    </dd>\n",
    "</dl>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pincone-example-index']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pinecone\n",
    "from tqdm.autonotebook import tqdm\n",
    "\n",
    "pinecone.init(api_key=os.environ.get(\"PINECONE_OPEN_API_KEY\"), environment=os.environ.get(\"PINECONE_ENVIORNMENT\"))\n",
    "#Creating an Index\n",
    "index_name=\"pincone-example-index\"\n",
    "#delete the index if an index of the same name already exists\n",
    "if index_name in pinecone.list_indexes():\n",
    "    pinecone.delete_index(index_name)\n",
    "#Create Index\n",
    "my_dimensions = 384 #needs to match the length of one of the upload vectors. Not the total number of vectors\n",
    "#distance metric\n",
    "distance_metric = \"cosine\"\n",
    "#shards\n",
    "shards_used=1\n",
    "pinecone.create_index(name=index_name, dimension=my_dimensions, metric=distance_metric, shards=shards_used)\n",
    "#connect to index\n",
    "index=pinecone.Index(index_name=index_name)\n",
    "#Display active Indexes\n",
    "pinecone.list_indexes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Vectorization</h2>\n",
    "<p>The next crucial step in the Pinecone workflow is vectorizing the data. Once the document is vectorized, it must be upserted into the Pinecone index. While individual upserts were used in our example, for larger applications, batching is recommended for enhanced performance and efficiency.</p>\n",
    "<dl>\n",
    "    <dt><strong>Batch Upsert:</strong></dt>\n",
    "    <dd>\n",
    "        <p>For substantial datasets, batch upserts are preferred over individual upserts. Pinecone efficiently supports batch upserts, allowing for a significant performance boost. You can customize the batch size to suit your specific data and resource requirements, optimizing the balance between performance and resource utilization.</p>\n",
    "    </dd>\n",
    "</dl>\n",
    "<p>Utilizing batch upserts effectively can significantly speed up the process of indexing large volumes of data, making your application more responsive and scalable.</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "# Path to your file\n",
    "file_path = \"D:\\Rag Retrival Zimm\\data\\What I Worked On.txt\"\n",
    "\n",
    "# Read the file\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    text_data = [line.strip() for line in file.readlines()]\n",
    "# Example of removing special characters (if necessary)\n",
    "\n",
    "processed_texts = [re.sub(r'[^A-Za-z0-9\\s]', '', text) for text in text_data]\n",
    "\n",
    "\n",
    "# Load the model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Convert text to vectors\n",
    "vectors = model.encode(processed_texts).tolist()\n",
    "assert len(vectors[1]) == my_dimensions\n",
    "#this is part i am talking about with commit from above\n",
    "# Initialize Pinecone (add your API key and environment)\n",
    "#pinecone.init(api_key=os.environ.get(\"PINECONE_OPEN_API_KEY\"), environment=\"us-west1-gcp\")\n",
    "\n",
    "\n",
    "# Upload vectors to Pinecone\n",
    "for i, vector in enumerate(vectors):\n",
    "    index.upsert(vectors=[(str(i), vector)])  # Using the index in the list as the ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Querying in Pinecone</h3>\n",
    "<p>After uploading the article into a Pinecone Index named <code>index</code>, we can begin querying. This process involves several steps:</p>\n",
    "<ol>\n",
    "    <li><strong>Create a Query</strong>: Formulate a question that we want to find answers to within the article.</li>\n",
    "    <li><strong>Vectorize the Query</strong>: Convert the query text into a vector using the same method used for the article text. This ensures that the query is in a format compatible with the Pinecone index.</li>\n",
    "    <li><strong>Query the Index</strong>: Send the vectorized query to the Pinecone index. The index compares this query vector against the vectors of the uploaded article segments.</li>\n",
    "    <li><strong>Retrieve Relevant Indexes</strong>: Pinecone returns the indexes of the article segments most similar to the query. These indexes correspond to the positions of the segments in the pre-vectorized text.</li>\n",
    "    <li><strong>Extract Relevant Text</strong>: Using the returned indexes, we can then extract the corresponding text segments from the article. These segments contain information that is relevant to the query, effectively answering the posed question.</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'matches': [{'id': '4', 'score': 0.402957916, 'values': []},\n",
      "             {'id': '350', 'score': 0.393699348, 'values': []},\n",
      "             {'id': '190', 'score': 0.380302459, 'values': []},\n",
      "             {'id': '200', 'score': 0.350367397, 'values': []},\n",
      "             {'id': '98', 'score': 0.321385, 'values': []}],\n",
      " 'namespace': '',\n",
      " 'usage': {'readUnits': 6}}\n",
      "Before college the two main things I worked on outside of school were writing and programming I didnt write essays I wrote what beginning writers were supposed to write then and probably still are short stories My stories were awful They had hardly any plot just characters with strong feelings which I imagined made them deep Thanks to Trevor Blackwell John Collison Patrick Collison Daniel Gackle Ralph Hazell Jessica Livingston Robert Morris and Harj Taggar for reading drafts of this In the print era the channel for publishing essays had been vanishingly small Except for a few officially anointed thinkers who went to the right parties in New York the only people allowed to publish essays were specialists writing about their specialties There were so many essays that had never been written because there had been no way to publish them Now they could be and I was going to write them 12 Over the next several years I wrote lots of essays about all kinds of different topics OReilly reprinted a collection of them as a book called Hackers  Painters after one of the essays in it I also worked on spam filters and did some more painting I used to have dinners for a group of friends every thursday night which taught me how to cook for groups And I bought another building in Cambridge a former candy factory and later twas said porn studio to use as an office There were plenty of earnest students too kids who could draw in high school and now had come to what was supposed to be the best art school in the country to learn to draw even better They tended to be confused and demoralized by what they found at RISD but they kept going because painting was what they did I was not one of the kids who could draw in high school but at RISD I was definitely closer to their tribe than the tribe of signature style seekers\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "# Your query\n",
    "query_text = \"What did the author do growing up?\"\n",
    "\n",
    "# Convert query text to a vector\n",
    "query_vector = model.encode([query_text]).tolist()\n",
    "\n",
    "# Query Pinecone index\n",
    "query_results = index.query(vector=query_vector, top_k=5,include_metadata=True) #Change top_k to see how open_ai responses will change with more information\n",
    "print(query_results)\n",
    "# Extract IDs of top segments from query results\n",
    "top_segment_ids = [match['id'] for match in query_results['matches']]\n",
    "\n",
    "# Retrieve the corresponding text segments\n",
    "# Assuming you have a mapping of IDs to text segments\n",
    "top_segments = [processed_texts[int(id)] for id in top_segment_ids]\n",
    "\n",
    "\n",
    "context = \" \".join(top_segments)\n",
    "print(context)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<p>The pertinent excerpts of text related to our query have been meticulously extracted from the larger document. By inputting these specific segments into a Large Language Model (LLM), we are now poised to generate a comprehensive and coherent response, effectively harnessing the advanced capabilities of AI-driven natural language processing.</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before college, the author worked on writing and programming outside of school. They wrote short stories and worked on programming projects.\n"
     ]
    }
   ],
   "source": [
    "prompt = f\"Context: {context}\\nQuestion: What did the author do growing up?\\nAnswer:\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "  ]\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>ChromaDB</h2>\n",
    "<p>This section outlines the steps for utilizing ChromaDB to store, process, and retrieve text data efficiently. The process involves setting up the ChromaDB client, managing collections for data storage, and querying the stored data for relevant information.</p>\n",
    "<dl>\n",
    "    <dt><strong>1. Installation</strong></dt>\n",
    "    <dd>Begin by installing the ChromaDB package using pip, which enables you to use the ChromaDB functionalities within your Python environment.</dd>\n",
    "    <dd><code>pip install chromadb</code></dd>\n",
    "    <dt><strong>2. Initialize Chroma Client</strong></dt>\n",
    "    <dd>Instantiate the ChromaDB client. This client acts as the primary interface for interacting with your ChromaDB, allowing you to create collections, add documents, and perform queries.</dd>\n",
    "    <dt><strong>3. Manage Collections</strong></dt>\n",
    "    <dd>Collections in ChromaDB are akin to tables in a traditional database. They are used to store your embeddings, documents, and any associated metadata. If a collection with the desired name already exists, it's deleted to ensure a fresh start. Subsequently, a new collection is created for storing the processed documents.</dd>\n",
    "    <dt><strong>4. Process and Add Documents</strong></dt>\n",
    "    <dd>Documents, typically text data, are read from a specified file. Special characters are removed, and the clean text is added to the ChromaDB collection. ChromaDB handles tokenization, embedding, and indexing of these documents automatically, streamlining the process of converting raw text into a structured, queryable format.</dd>\n",
    "    <dt><strong>5. Query the Collection</strong></dt>\n",
    "    <dd>Once the documents are stored and indexed in the collection, you can perform queries using natural language questions or statements. ChromaDB returns the most relevant documents based on the similarity to the query, providing a powerful tool for information retrieval and insight extraction from your text corpus.</dd>\n",
    "</dl>\n",
    "<p>Through these steps, ChromaDB simplifies the process of managing and querying large sets of text data, making it an invaluable tool for data retrieval and analysis tasks.</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Before college the two main things I worked on outside of school were writing and programming I didnt write essays I wrote what beginning writers were supposed to write then and probably still are short stories My stories were awful They had hardly any plot just characters with strong feelings which I imagined made them deep',\n",
       "  'Thanks to Trevor Blackwell John Collison Patrick Collison Daniel Gackle Ralph Hazell Jessica Livingston Robert Morris and Harj Taggar for reading drafts of this',\n",
       "  'In the print era the channel for publishing essays had been vanishingly small Except for a few officially anointed thinkers who went to the right parties in New York the only people allowed to publish essays were specialists writing about their specialties There were so many essays that had never been written because there had been no way to publish them Now they could be and I was going to write them 12',\n",
       "  'Over the next several years I wrote lots of essays about all kinds of different topics OReilly reprinted a collection of them as a book called Hackers  Painters after one of the essays in it I also worked on spam filters and did some more painting I used to have dinners for a group of friends every thursday night which taught me how to cook for groups And I bought another building in Cambridge a former candy factory and later twas said porn studio to use as an office',\n",
       "  'There were plenty of earnest students too kids who could draw in high school and now had come to what was supposed to be the best art school in the country to learn to draw even better They tended to be confused and demoralized by what they found at RISD but they kept going because painting was what they did I was not one of the kids who could draw in high school but at RISD I was definitely closer to their tribe than the tribe of signature style seekers']]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import chromadb\n",
    "import re\n",
    "#Step 2\n",
    "chroma_client = chromadb.Client()\n",
    "\n",
    "#Step 3\n",
    "collection_name = \"Testing_GZ_123\"\n",
    "try:\n",
    "     chroma_client.delete_collection(name=collection_name)#delete client if already in exsitence this will not alway need to be done\n",
    "except:\n",
    "     pass\n",
    "\n",
    "# Create a new collection (either it didn't exist or it was just deleted)\n",
    "collection = chroma_client.create_collection(name=collection_name)\n",
    "\n",
    "\n",
    "#Step 4 add document \n",
    "# Path to your file\n",
    "file_path = \"D:\\Rag Retrival Zimm\\data\\What I Worked On.txt\"\n",
    "\n",
    "# Read the file\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    text_data = [line.strip() for line in file.readlines()]\n",
    "# Example of removing special characters (if necessary)\n",
    "\n",
    "processed_texts = [re.sub(r'[^A-Za-z0-9\\s]', '', text) for text in text_data]\n",
    "#Add text to collection\n",
    "collection.add(\n",
    "    documents=processed_texts,\n",
    "    ids=[str(i) for i in range(1,(len(processed_texts)+1))]\n",
    ")\n",
    "#Step 5: Query\n",
    "results_chromadb = collection.query(\n",
    "    query_texts=[\"What did the author do growing up?\"],\n",
    "    n_results=5\n",
    ")\n",
    "chroma_context=results_chromadb[\"documents\"]\n",
    "#Viewing the Relevant Text\n",
    "chroma_context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<p>The pertinent excerpts of text related to our query have been meticulously extracted from the larger document. By inputting these specific segments into a Large Language Model (LLM), we are now poised to generate a comprehensive and coherent response, effectively harnessing the advanced capabilities of AI-driven natural language processing.</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Growing up, the author worked on writing and programming outside of school. They wrote short stories, although they felt that their stories were not very good. They also mentioned working on spam filters and engaging in painting. Additionally, the author used to host dinners for friends and bought a building in Cambridge to use as an office.\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "prompt = f\"Context: {chroma_context}\\nQuestion: What did the author do growing up?\\nAnswer:\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are helping someone answer the question\"},\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "  ]\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
