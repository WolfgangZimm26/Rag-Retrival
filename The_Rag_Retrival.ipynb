{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"text-align:center\">Garrett Zimmerman</h3>\n",
    "<h3 style=\"text-align:center\">January 6, 2024</h3>\n",
    "<h1 style=\"font-weight:bold;text-align:center\">Concepts of Rag  Retrieval</h1>\n",
    "<h2 style=\"font-weight:bold\">Goal:</h2>\n",
    "<p>\n",
    "<dl>\n",
    "    <dt style=\"font-weight:bold\">RAG Retrieval:</dt> \n",
    "    <dd>Review the concept of RAG Retrieval<dd> \n",
    "    <dd>Question and Answering vs. Fine Tuning a Model<dd>\n",
    "    <dd>Advanced  Retrieval vs. Basic Retrieval</dd>\n",
    "    <dt style=\"font-weight:bold\">Build Several Examples:</dt> \n",
    "    <dd>Explain different advance retrieval methods\n",
    "        <ul>\n",
    "            <li>LlamaIndex</li>\n",
    "            <li>LangChain</li>\n",
    "            <li>Pinecone</li>\n",
    "            <li>ChromaDB</li>\n",
    "        </ul>\n",
    "    </dd> \n",
    "</dl>\n",
    "</p> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"font-weight:bold\">RAG (Retrieval-Augmented Generation) Retrieval:</h2> \n",
    "<p>A method used in natural language processing that combines the retrieval of relevant documents with a generative language model to enhance the model's ability to provide informative and accurate responses. This method is particularly useful for question-answering tasks and other applications where having access to a broad range of external information is beneficial.</p> \n",
    "<h3>Here's a breakdown of how RAG retrieval works</h3>\n",
    "<dl>\n",
    "    <dt>1. Retrieval Component</dt> \n",
    "    <dd>The first step in RAG retrieval involves retrieving relevant documents or information. When a query or question is posed, the system searches a large database of texts to find the most relevant documents. This database can be anything from a simple collection of articles to a comprehensive knowledge base.</dd>\n",
    "    <dt>2. Generative Component</dt> \n",
    "    <dd>Once the relevant documents are retrieved, a generative language model, like GPT (Generative Pretrained Transformer), is used. This model takes the input query and the retrieved documents as context to generate a response.</dd>\n",
    "    <dt>3. Combination of Retrieval and Generation</dt> \n",
    "    <dd>The key aspect of RAG retrieval is how it combines these two components. The retrieved documents provide the model with specific, detailed information relevant to the query, which the model might not have in its pre-trained knowledge. The generative model then synthesizes this information to create a coherent and contextually appropriate response.<dd>\n",
    "    <dt>3. Benefits<dt> \n",
    "    <dd>This approach allows the language model to answer questions or provide information that is more up-to-date, detailed, and specific than what it could generate based solely on its pre-trained knowledge. It's particularly useful for queries where the answer might not be common knowledge or is very specific.</dd>\n",
    "    <dt>4. Applications</dt> \n",
    "    <dd>RAG retrieval is commonly used in advanced chatbots, question-answering systems, and research tools. It's especially valuable in situations where keeping up with the latest information or covering a vast range of topics is essential.</dd>\n",
    "    <dt>Example:</dt> \n",
    "    <dd>Suppose someone asks a question about a recent scientific discovery. A RAG retrieval system would first find relevant scientific articles or papers about that discovery, then use a generative model to construct an answer that accurately reflects the current understanding as presented in those documents</dd>\n",
    "</dl>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"font-weight:bold\">Question and Answering vs. Fine-Tuning a Model</h2>\n",
    "<h3>Question and Answering (Q&A):</h3>\n",
    "<dl>\n",
    "    <dt>Definition:</dt> \n",
    "    <dd>Question and Answering in the context of AI and language models refers to the process where a model responds to queries posed in natural language. The goal is to provide accurate, relevant, and concise answers based on the model's training and knowledge.</dd>\n",
    "    <dt>Usage:<dt> \n",
    "    <dd>Q&A systems are widely used in chatbots, virtual assistants, and information retrieval systems. They are designed to understand a wide range of questions and provide answers that are drawn from their training data or real-time data sources.</dd>\n",
    "    <dt>Mechanism:</dt> \n",
    "    <dd>These systems typically use pre-trained models like GPT-3, which have been trained on vast amounts of text data, enabling them to generate responses based on patterns and information they have learned.</dd>\n",
    "<dl>\n",
    "<h3>Fine-Tuning a Model:</h3>\n",
    "<dl>\n",
    "    <dt>Definition:</dt> \n",
    "    <dd>Fine-tuning refers to the process of taking a pre-trained language model and further training it on a specific dataset to specialize its responses for a particular domain or task.</dd>\n",
    "    <dt>Usage:</dt> \n",
    "    <dd>Fine-tuning is common when you need a model to perform well on a specific type of data or task that was not the primary focus of the original, broader training. For instance, fine-tuning a model for medical Q&A, legal advice, or technical support.</dd>\n",
    "    <dt>Mechanism:</dt>\n",
    "    <dd>During fine-tuning, the model's weights are slightly adjusted so that it becomes more adept at understanding and generating responses relevant to the specialized domain. This process requires a smaller dataset and less computational power compared to training a model from scratch.</dd>\n",
    "</dl>\n",
    "<h3>Summary:</h3>\n",
    "<p>Q&A systems provide direct answers to user queries using pre-trained or fine-tuned models, while fine-tuning tailors a model to specific domains or tasks</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"font-weight: bold\">Advanced Retrieval vs. Basic Retrieval<h2>\n",
    "<h3>Basic Retrieval:</h3>\n",
    "<dl>\n",
    "    <dt>Definition:</dt> \n",
    "    <dd>Basic retrieval involves straightforward methods of finding information in response to a query. This often involves keyword matching, where the system looks for documents or data entries that contain the same words or phrases as the query.</dd>\n",
    "    <dt>Limitations:</dt> \n",
    "    <dd>Basic retrieval can be limited in handling complex queries, understanding context, or providing nuanced responses. It's generally less effective when dealing with ambiguous or multi-faceted questions.</dd>\n",
    "</dl>\n",
    "<h3>Advanced Retrieval:</h3>\n",
    "<dl>\n",
    "    <dt>Definition:</dt> \n",
    "    <dd>Advanced retrieval encompasses more sophisticated techniques that go beyond simple keyword matching. These methods might include semantic search, contextual understanding, and the integration of AI models.</dd>\n",
    "    <dt>Features:\n",
    "        <ul>\n",
    "            <dl>\n",
    "                <li><dt>Semantic Understanding:</dt></li> \n",
    "                <dd>Advanced systems understand the meaning behind words in a query, allowing them to retrieve information that is conceptually related, even if it doesn’t contain the exact keywords.</dd>\n",
    "                <li><dt>Context Awareness:</dt></li> \n",
    "                <dd>They can consider the context of a query, providing more relevant and precise results. For example, understanding the user's previous queries or the broader topic at hand.</dd>\n",
    "                <li><dt>Integration with AI:</dt></li> \n",
    "                <dd>Advanced retrieval often involves the use of AI models like neural networks, which can process and understand natural language at a more sophisticated level.</dd>\n",
    "            </dl>\n",
    "       </ul> \n",
    "    </dt>\n",
    "    <dt>Applications:</dt> \n",
    "    <dd>Advanced retrieval is essential in complex domains where queries require deep understanding and nuanced responses, such as in legal research, academic literature search, and specialized information databases.</dd>\n",
    "</dl>\n",
    "<h3>Summary:</h3>\n",
    "<p>Basic retrieval relies on simple keyword matching, whereas advanced retrieval uses more sophisticated techniques for a deeper understanding and contextual awareness in information retrieval</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"font-weight:bold\">Examples</h2>\n",
    "<h3 style=\"font-weight:bold\">LlamaIndex</h3>\n",
    "<p>LlamaIndex uses RAG Retrival. By using RAG Retrival LlamaIndex overcomes some weaknesses of the fine tuning approach:</p>\n",
    "<ul>\n",
    "    <li>There’s no training involved, so it’s cheap</li>\n",
    "    <li>Data is fetched only when you ask for them, so it’s always up to date</li>\n",
    "    <li>LlamaIndex can show you the retrieved documents, so it’s more trustworthy</li>\n",
    "</ul>\n",
    "<h4 style=\"font-weight:bold\">Tools Provided by LlamaIndex</h4>\n",
    "<dl>\n",
    "    <dt>Data Connectors</dt> \n",
    "    <dd>Ingest your existing data from their native source and format. These could be APIs, PDFs, SQL, and (much) more</dd>\n",
    "    <dt>Data Indexes</dt> \n",
    "    <dd>Structure your data in intermediate representations that are easy and performant for LLMs to consume</dd>\n",
    "    <dt>Engines provide natural language access to your data</dt>\n",
    "    <dd>For example: Query engines are powerful retrieval interfaces for knowledge-augmented output</dd>\n",
    "    <dd>For example: Chat engines are conversational interfaces for multi-message, “back and forth” interactions with your data</dd>\n",
    "    <dt>Data Agents</dt>\n",
    "    <dd>LLM-powered knowledge workers augmented by tools, from simple helper functions to API integrations and more</dd>\n",
    "    <dt>Application Integrations</dt>\n",
    "    <dd>Tie LlamaIndex back into the rest of your ecosystem. This could be LangChain, Flask, Docker, ChatGPT, or… anything else<dd>\n",
    "</dl>\n",
    "<h4>Documentation</h4>\n",
    "<p>To find out more about LlamaIndex and to see documentation please visit: <a href=https://docs.llamaindex.ai/en/stable>LlamaIndexDoc</a></p>\n",
    "<h4>Getting Started in LlamaIndex</h4>\n",
    "<p>Run this Line in the terminal to install LlamaIndex: pip install llama-index</p>\n",
    "<p>In the following example we are using LlamaIndex to find out information about the author in a currently written piece. Make sure the folder data is in the same directory of the python script or the Jupyter Notebook for this script to work. If the data file is not in the same directory make sure to update to file location</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting Imports and Uploading API Key. The API key should be stored in a the same directory and named .env for this script to work. If API key is stored elsewhere update code to find file location  \n",
    "import nltk\n",
    "from llama_index import (\n",
    "    VectorStoreIndex,\n",
    "    SimpleDirectoryReader,\n",
    "    StorageContext,\n",
    "    load_index_from_storage,\n",
    ")\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "# Load the environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "#API Keys and Envriornments we will need in this notebook.\n",
    "required_env_vars = {\n",
    "    \"OPENAI_API_KEY\": \"OPENAI_API_KEY\",\n",
    "    \"PINECONE_OPEN_API_KEY\": \"PINECONE_OPEN_API_KEY\",\n",
    "    \"PINECONE_ENVIORNMENT\": \"PINECONE_ENVIORNMENT\"\n",
    "}\n",
    "# Iterate through the required environment variables\n",
    "for var, name in required_env_vars.items():\n",
    "    value = os.environ.get(var)\n",
    "    if value is None:\n",
    "        raise ValueError(f\"{name} is not set in the environment variables\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The author mentioned that before college, they worked on writing and programming. They wrote short stories and tried writing programs on the IBM 1401 computer. They also mentioned getting a microcomputer, a TRS-80, and started programming on it. They wrote simple games, a program to predict rocket heights, and even a word processor.\n"
     ]
    }
   ],
   "source": [
    "# check if storage already exists\n",
    "PERSIST_DIR = \"./storage\"\n",
    "if not os.path.exists(PERSIST_DIR):\n",
    "    # load the documents and create the index\n",
    "    documents = SimpleDirectoryReader(\"data\").load_data()\n",
    "    index = VectorStoreIndex.from_documents(documents)\n",
    "    # store it for later\n",
    "    index.storage_context.persist(persist_dir=PERSIST_DIR)\n",
    "else:\n",
    "    # load the existing index\n",
    "    storage_context = StorageContext.from_defaults(persist_dir=PERSIST_DIR)\n",
    "    index = load_index_from_storage(storage_context)\n",
    "\n",
    "# either way we can now query the index\n",
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(\"What did the author do growing up?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"font-weight:bold\">LangChain</h3>\n",
    "<p>LangChain is a framework for developing applications powered by language models. It enables applications that:\n",
    "<ul>\n",
    "<li>Are context-aware: connect a language model to sources of context (prompt instructions, few shot examples, content to ground its response in, etc.)</li>\n",
    "<li>Reason: rely on a language model to reason (about how to answer based on provided context, what actions to take, etc.)</li>\n",
    "</ul>\n",
    "This framework consists of LangChain Libraries, LangChain Templates, LangServe, and LangSmith.</p>\n",
    "<p>Together, these products simplify the entire application lifecycle:\n",
    "<dl>\n",
    "    <dt>Develop:</dt> \n",
    "    <dd>Write your applications in LangChain/LangChain.js. Hit the ground running using Templates for reference.</dd>\n",
    "    <dt>Productionize:</dt> \n",
    "    <dd>Use LangSmith to inspect, test and monitor your chains, so that you can constantly improve and deploy with confidence.</dd>\n",
    "    <dt>Deploy:</dt> \n",
    "    <dd>Turn any chain into an API with LangServe.</dd>\n",
    "</dl>\n",
    "For more information and documentaion of LangChain visit: <a href=https://python.langchain.com/docs/get_started/introduction>LangChain Doc</a></p>\n",
    "<p>To start make sure to install LangChain using : pip install langchain</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>LangChain Steps</h2>\n",
    "<dl>\n",
    "    <dt>1. Document Loaders</dt>\n",
    "    <dd>Use document loaders to load data from a source as Document's. A Document is a piece of text and associated metadata. For example, there are document loaders for loading a simple .txt file, for loading the text contents of any web page, or even for loading a transcript of a YouTube video. Document loaders provide a \"load\" method for loading data as documents from a configured source. They optionally implement a \"lazy load\" as well for lazily loading data into memory.</dd>\n",
    "    <dt>2. Text Splitters</dt>\n",
    "    <dd>Once you've loaded documents, you'll often want to transform them to better suit your application. The simplest example is you may want to split a long document into smaller chunks that can fit into your model's context window. LangChain has a number of built-in document transformers that make it easy to split, combine, filter, and otherwise manipulate documents. When you want to deal with long pieces of text, it is necessary to split up that text into chunks. As simple as this sounds, there is a lot of potential complexity here. Ideally, you want to keep the semantically related pieces of text together. What \"semantically related\" means could depend on the type of text. At a high level, text splitters work as following:\n",
    "        <ol>\n",
    "        <li> Split the text up into small, semantically meaningful chunks (often sentences).</li>\n",
    "        <li>Start combining these small chunks into a larger chunk until you reach a certain size (as measured by some function).</li>\n",
    "        <li>Once you reach that size, make that chunk its own piece of text and then start creating a new chunk of text with some overlap (to keep context between chunks).</li>\n",
    "        </ol>\n",
    "        That means there are two different axes along which you can customize your text splitter:\n",
    "        <ol>\n",
    "        <li>How the text is split</li>\n",
    "        <li>How the chunk size is measured</li>\n",
    "        </ol>\n",
    "</dl>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "#pip3 install beautifulsoup4\n",
    "#Step 1 Load the document\n",
    "with open(\"D:\\Rag Retrival Zimm\\data\\What I Worked On.txt\", 'r') as file:\n",
    "    document_text = file.read()\n",
    "#Step 1.5 Cleaning the Document of HTML\n",
    "soup = BeautifulSoup(document_text, 'html.parser')\n",
    "clean_text = soup.get_text()\n",
    "# Remove newline characters\n",
    "clean_text = clean_text.replace('\\n', ' ')  # Replace with a space or '' to remove entirely\n",
    "# Replace the escaped single quote\n",
    "clean_text = clean_text.replace(\"\\\\'\", \"'\")\n",
    "display(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Pinecone</h2>\n",
    "<p>Pinecone makes it easy to provide long-term memory for high-performance AI applications. It’s a managed, cloud-native vector database with a simple API and no infrastructure hassles. Pinecone serves fresh, filtered query results with low latency at the scale of billions of vectors.</p>\n",
    "<ul>\n",
    "<li>Vector embeddings provide long-term memory for AI</li>\n",
    "<li>Vector databases store and query embeddings quickly and at scale</li>\n",
    "</ul>\n",
    "<h3>Install Pinecone: pip install pinecone-client </h3>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"font-weight:bold\">Creating an Index</h3>\n",
    "<p>Think of a Pinecone Index as a database that stores and organizes our information for efficient retrieval.</p>\n",
    "\n",
    "<h3>Distance Metrics</h3>\n",
    "<p>When creating a vector index in Pinecone, you can choose from different distance metrics, each with its unique characteristics:</p>\n",
    "\n",
    "<dl>\n",
    "    <dt><strong>Euclidean</strong></dt>\n",
    "    <dd>\n",
    "        <ul>\n",
    "            <li>Used to calculate the straight-line distance between two points. It's a common choice for spatial data.</li>\n",
    "            <li>With 'metric=euclidean', closer vectors have lower scores, indicating higher similarity.</li>\n",
    "        </ul>\n",
    "    </dd>\n",
    "    <dt><strong>Cosine</strong></dt>\n",
    "    <dd>\n",
    "        <ul>\n",
    "            <li>Commonly used for text data, measuring the cosine of the angle between vectors. It normalizes scores to a [-1, 1] range.</li>\n",
    "        </ul>\n",
    "    </dd>\n",
    "    <dt><strong>Dot Product</strong></dt>\n",
    "    <dd>\n",
    "        <ul>\n",
    "            <li>Calculates the product of two vectors, indicating how similar they are in direction. Higher positive scores denote greater similarity.</li>\n",
    "        </ul>\n",
    "    </dd>\n",
    "</dl>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pincone-example-index']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pinecone\n",
    "from tqdm.autonotebook import tqdm\n",
    "\n",
    "pinecone.init(api_key=os.environ.get(\"PINECONE_OPEN_API_KEY\"), environment=os.environ.get(\"PINECONE_ENVIORNMENT\"))\n",
    "#Creating an Index\n",
    "index_name=\"pincone-example-index\"\n",
    "#delete the index if an index of the same name already exists\n",
    "if index_name in pinecone.list_indexes():\n",
    "    pinecone.delete_index(index_name)\n",
    "#Create Index\n",
    "my_dimensions = 384 #needs to match the length of one of the upload vectors. Not the total number of vectors\n",
    "#distance metric\n",
    "distance_metric = \"cosine\"\n",
    "#shards\n",
    "shards_used=1\n",
    "pinecone.create_index(name=index_name, dimension=my_dimensions, metric=distance_metric, shards=shards_used)\n",
    "#connect to index\n",
    "index=pinecone.Index(index_name=index_name)\n",
    "#Display active Indexes\n",
    "pinecone.list_indexes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Vectorization</h2>\n",
    "<p>The next crucial step in the Pinecone workflow is vectorizing the data. Once the document is vectorized, it must be upserted into the Pinecone index. While individual upserts were used in our example, for larger applications, batching is recommended for enhanced performance and efficiency.</p>\n",
    "<dl>\n",
    "    <dt><strong>Batch Upsert:</strong></dt>\n",
    "    <dd>\n",
    "        <p>For substantial datasets, batch upserts are preferred over individual upserts. Pinecone efficiently supports batch upserts, allowing for a significant performance boost. You can customize the batch size to suit your specific data and resource requirements, optimizing the balance between performance and resource utilization.</p>\n",
    "    </dd>\n",
    "</dl>\n",
    "<p>Utilizing batch upserts effectively can significantly speed up the process of indexing large volumes of data, making your application more responsive and scalable.</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "# Path to your file\n",
    "file_path = \"D:\\Rag Retrival Zimm\\data\\What I Worked On.txt\"\n",
    "\n",
    "# Read the file\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    text_data = [line.strip() for line in file.readlines()]\n",
    "# Example of removing special characters (if necessary)\n",
    "\n",
    "processed_texts = [re.sub(r'[^A-Za-z0-9\\s]', '', text) for text in text_data]\n",
    "\n",
    "\n",
    "# Load the model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Convert text to vectors\n",
    "vectors = model.encode(processed_texts).tolist()\n",
    "assert len(vectors[1]) == my_dimensions\n",
    "#this is part i am talking about with commit from above\n",
    "# Initialize Pinecone (add your API key and environment)\n",
    "#pinecone.init(api_key=os.environ.get(\"PINECONE_OPEN_API_KEY\"), environment=\"us-west1-gcp\")\n",
    "\n",
    "\n",
    "# Upload vectors to Pinecone\n",
    "for i, vector in enumerate(vectors):\n",
    "    index.upsert(vectors=[(str(i), vector)])  # Using the index in the list as the ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Querying in Pinecone</h3>\n",
    "<p>After uploading the article into a Pinecone Index named <code>index</code>, we can begin querying. This process involves several steps:</p>\n",
    "<ol>\n",
    "    <li><strong>Create a Query</strong>: Formulate a question that we want to find answers to within the article.</li>\n",
    "    <li><strong>Vectorize the Query</strong>: Convert the query text into a vector using the same method used for the article text. This ensures that the query is in a format compatible with the Pinecone index.</li>\n",
    "    <li><strong>Query the Index</strong>: Send the vectorized query to the Pinecone index. The index compares this query vector against the vectors of the uploaded article segments.</li>\n",
    "    <li><strong>Retrieve Relevant Indexes</strong>: Pinecone returns the indexes of the article segments most similar to the query. These indexes correspond to the positions of the segments in the pre-vectorized text.</li>\n",
    "    <li><strong>Extract Relevant Text</strong>: Using the returned indexes, we can then extract the corresponding text segments from the article. These segments contain information that is relevant to the query, effectively answering the posed question.</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'matches': [{'id': '4', 'score': 0.402957916, 'values': []},\n",
      "             {'id': '350', 'score': 0.393699348, 'values': []},\n",
      "             {'id': '190', 'score': 0.380302459, 'values': []},\n",
      "             {'id': '200', 'score': 0.350367397, 'values': []},\n",
      "             {'id': '98', 'score': 0.321385, 'values': []}],\n",
      " 'namespace': '',\n",
      " 'usage': {'readUnits': 6}}\n",
      "['Before college the two main things I worked on outside of school were writing and programming I didnt write essays I wrote what beginning writers were supposed to write then and probably still are short stories My stories were awful They had hardly any plot just characters with strong feelings which I imagined made them deep', 'Thanks to Trevor Blackwell John Collison Patrick Collison Daniel Gackle Ralph Hazell Jessica Livingston Robert Morris and Harj Taggar for reading drafts of this', 'In the print era the channel for publishing essays had been vanishingly small Except for a few officially anointed thinkers who went to the right parties in New York the only people allowed to publish essays were specialists writing about their specialties There were so many essays that had never been written because there had been no way to publish them Now they could be and I was going to write them 12', 'Over the next several years I wrote lots of essays about all kinds of different topics OReilly reprinted a collection of them as a book called Hackers  Painters after one of the essays in it I also worked on spam filters and did some more painting I used to have dinners for a group of friends every thursday night which taught me how to cook for groups And I bought another building in Cambridge a former candy factory and later twas said porn studio to use as an office', 'There were plenty of earnest students too kids who could draw in high school and now had come to what was supposed to be the best art school in the country to learn to draw even better They tended to be confused and demoralized by what they found at RISD but they kept going because painting was what they did I was not one of the kids who could draw in high school but at RISD I was definitely closer to their tribe than the tribe of signature style seekers']\n",
      "Before college the two main things I worked on outside of school were writing and programming I didnt write essays I wrote what beginning writers were supposed to write then and probably still are short stories My stories were awful They had hardly any plot just characters with strong feelings which I imagined made them deep Thanks to Trevor Blackwell John Collison Patrick Collison Daniel Gackle Ralph Hazell Jessica Livingston Robert Morris and Harj Taggar for reading drafts of this In the print era the channel for publishing essays had been vanishingly small Except for a few officially anointed thinkers who went to the right parties in New York the only people allowed to publish essays were specialists writing about their specialties There were so many essays that had never been written because there had been no way to publish them Now they could be and I was going to write them 12 Over the next several years I wrote lots of essays about all kinds of different topics OReilly reprinted a collection of them as a book called Hackers  Painters after one of the essays in it I also worked on spam filters and did some more painting I used to have dinners for a group of friends every thursday night which taught me how to cook for groups And I bought another building in Cambridge a former candy factory and later twas said porn studio to use as an office There were plenty of earnest students too kids who could draw in high school and now had come to what was supposed to be the best art school in the country to learn to draw even better They tended to be confused and demoralized by what they found at RISD but they kept going because painting was what they did I was not one of the kids who could draw in high school but at RISD I was definitely closer to their tribe than the tribe of signature style seekers\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "# Your query\n",
    "query_text = \"What did the author do growing up?\"\n",
    "\n",
    "# Convert query text to a vector\n",
    "query_vector = model.encode([query_text]).tolist()\n",
    "\n",
    "# Query Pinecone index\n",
    "query_results = index.query(vector=query_vector, top_k=5,include_metadata=True) #Change top_k to see how open_ai responses will change with more information\n",
    "print(query_results)\n",
    "# Extract IDs of top segments from query results\n",
    "top_segment_ids = [match['id'] for match in query_results['matches']]\n",
    "\n",
    "# Retrieve the corresponding text segments\n",
    "# Assuming you have a mapping of IDs to text segments\n",
    "top_segments = [processed_texts[int(id)] for id in top_segment_ids]\n",
    "print(top_segments)\n",
    "\n",
    "context = \" \".join(top_segments)\n",
    "print(context)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<p>The pertinent excerpts of text related to our query have been meticulously extracted from the larger document. By inputting these specific segments into a Large Language Model (LLM), we are now poised to generate a comprehensive and coherent response, effectively harnessing the advanced capabilities of AI-driven natural language processing.</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before college, the author worked on writing and programming outside of school. They wrote short stories and worked on programming projects.\n"
     ]
    }
   ],
   "source": [
    "prompt = f\"Context: {context}\\nQuestion: What did the author do growing up?\\nAnswer:\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "  ]\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
